# Import Libraries
import pandas as pd  # Import pandas for data manipulation and analysis
import numpy as np  # Import numpy for numerical operations
import nltk  # Import nltk for natural language processing
import matplotlib.pyplot as plt  # Import matplotlib for creating visualizations
import seaborn as sns  # Import seaborn for statistical data visualization
import pickle  # Import pickle for saving and loading Python objects
from nltk.corpus import stopwords  # Import stopwords to filter out common words
from nltk.tokenize import word_tokenize  # Import word_tokenize to split text into tokens
from nltk.stem import WordNetLemmatizer  # Import WordNetLemmatizer for reducing words to their base form
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV  # Import functions for model evaluation and tuning
from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer to convert text data into numerical features
from sklearn.linear_model import LogisticRegression  # Import LogisticRegression for classification
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Import metrics to evaluate model performance
from sklearn.pipeline import make_pipeline  # Import make_pipeline to streamline the process of applying transformations and modeling

# Download necessary NLTK data
nltk.download('punkt')  # Download tokenizer data for word_tokenize
nltk.download('stopwords')  # Download stopwords data to remove common words
nltk.download('wordnet')  # Download WordNet data for lemmatization

# Load dataset
data = pd.read_csv('/content/Sentiment_Analysis_Dataset.csv', encoding='ISO-8859-1')  # Load the dataset from a CSV file

# Display dataset info and label distribution
print(data.head())  # Print the first few rows of the dataset to understand its structure
print(data['label'].value_counts())  # Print the distribution of labels to understand class balance

# Preprocess text function
def preprocess_text(text):
    lemmatizer = WordNetLemmatizer()  # Initialize the lemmatizer
    tokens = word_tokenize(text.lower())  # Tokenize and convert text to lowercase
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stopwords.words('english')]  # Lemmatize tokens, remove non-alphanumeric tokens and stopwords
    return ' '.join(tokens)  # Join tokens back into a single string

# Apply preprocessing to the text column
data['cleaned_text'] = data['text'].apply(preprocess_text)  # Apply the preprocessing function to the text column

# Split data into training and testing sets
X = data['cleaned_text']  # Feature set (cleaned text)
y = data['label']  # Target variable (sentiment labels)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split data into training and testing sets (80% training, 20% testing)

# Create and train model using TfidfVectorizer and Logistic Regression
pipeline = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))  # Create a pipeline with TfidfVectorizer and Logistic Regression

# Cross-validation
cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')  # Perform cross-validation to evaluate model performance
print(f"Cross-Validation Accuracy Scores: {cv_scores}")  # Print cross-validation scores
print(f"Mean Cross-Validation Accuracy: {np.mean(cv_scores):.3f}")  # Print mean cross-validation accuracy

# Hyperparameter Tuning
param_grid = {
    'tfidfvectorizer__max_features': [1000, 5000, 10000, 20000],  # Grid of values for maximum number of features
    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2)],  # Grid of values for n-gram range
    'logisticregression__C': [0.1, 1, 10, 100]  # Grid of values for regularization parameter
}
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)  # Initialize GridSearchCV for hyperparameter tuning
grid_search.fit(X_train, y_train)  # Fit GridSearchCV on the training data
print("Best Parameters:", grid_search.best_params_)  # Print the best hyperparameters found
best_pipeline = grid_search.best_estimator_  # Get the best pipeline with tuned hyperparameters

# Evaluate Best Model
y_pred = best_pipeline.predict(X_test)  # Predict sentiments on the test set
accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy of the model
print("Accuracy:", accuracy)  # Print accuracy score
print(classification_report(y_test, y_pred))  # Print classification report showing precision, recall, and F1-score

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)  # Compute confusion matrix
plt.figure(figsize=(10, 7))  # Set figure size for the plot
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])  # Plot confusion matrix
plt.xlabel('Predicted')  # Label x-axis
plt.ylabel('True')  # Label y-axis
plt.title('Confusion Matrix')  # Set title of the plot
plt.show()  # Display the plot

# Visualization of Sentiment Distribution in the Test Set
sentiment_counts = pd.Series(y_pred).value_counts()  # Count occurrences of each sentiment in predictions
plt.figure(figsize=(8, 5))  # Set figure size for the plot
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')  # Plot bar chart for sentiment distribution
plt.xlabel('Sentiment')  # Label x-axis
plt.ylabel('Count')  # Label y-axis
plt.title('Sentiment Distribution in Predictions')  # Set title of the plot
plt.xticks(ticks=[0, 1, 2], labels=['Negative', 'Neutral', 'Positive'])  # Set x-tick labels
plt.show()  # Display the plot

# Save the trained model using pickle
with open('sentiment_model.pkl', 'wb') as model_file:  # Open a file in write-binary mode
    pickle.dump(best_pipeline, model_file)  # Save the trained model to the file

print("Model saved as 'sentiment_model.pkl'.")  # Print confirmation message

# Save the TfidfVectorizer separately
with open('tfidf_vectorizer.pkl', 'wb') as file:  # Open a file in write-binary mode
    pickle.dump(best_pipeline.named_steps['tfidfvectorizer'], file)  # Save the TfidfVectorizer to the file

print("TfidfVectorizer saved as 'tfidf_vectorizer.pkl'.")  # Print confirmation message

# Example rule-based post-processing (optional)
def custom_sentiment_classification(text, model_prediction):
    # Example of checking for mixed sentiment words
    mixed_words = ['good', 'bad']  # Add more mixed sentiment words as necessary
    tokens = word_tokenize(text.lower())  # Tokenize the input text
    if any(word in tokens for word in mixed_words):
        return 'Neutral'  # Override to Neutral if both positive and negative words exist
    else:
        # Map numeric predictions to sentiment labels
        sentiment = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}
        return sentiment.get(model_prediction, 'Unknown')

# Adjust predict_sentiment to use the custom logic
def predict_sentiment(text):
    processed_text = preprocess_text(text)  # Preprocess the input text
    model_prediction = best_pipeline.predict([processed_text])[0]  # Get model prediction
    final_prediction = custom_sentiment_classification(text, model_prediction)  # Apply custom rule
    return final_prediction

# Example usage
user_input = input("Enter a sentence for sentiment analysis: ")
print("Predicted Sentiment:", predict_sentiment(user_input))
